{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"homework.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gCIvz30AOj-H"},"source":["# 作業 : 實做向量拼接方式 Attention\n","***\n","## [作業目標]\n","實做向量拼接方式 Attention 運用於之前的 Seq2Seq attention\n","\n","## [作業重點]\n","向量拼接方式 Attention:\n","- 先將 q and k concat 起來\n","- 經過一層 W 參數(attention) 和 tanh activation function\n","- 最後乘上 Vt 調整成一個同等於 input 的數列\n","\n","$ \\qquad R(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^\\top\\tanh\\left(\\mathbf{W}_a[\\mathbf{q};\\mathbf{k}]\\right)$"]},{"cell_type":"code","metadata":{"id":"UIBD2Nn-OI-1","executionInfo":{"status":"ok","timestamp":1612277596879,"user_tz":-480,"elapsed":10014,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import os\n","import re\n","import csv\n","import math\n","import time\n","import spacy\n","import random\n","import pandas as pd\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from sklearn.model_selection import train_test_split\n","from torchtext.data import Field, BucketIterator, TabularDataset\n","from torchtext.data.metrics import bleu_score\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQoAR8K-RyHd","executionInfo":{"status":"ok","timestamp":1612277598941,"user_tz":-480,"elapsed":12065,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"0057f5b0-292b-4283-bf5c-eb388982a6cc"},"source":["# 進行 matplotlib 繪製中文\n","# 下載字體\n","!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.font_manager import FontProperties\n","\n","# 自定義字體\n","myfont = FontProperties(fname='taipei_sans_tc_beta.ttf')\n","sns.set()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-02-02 14:53:16--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n","Resolving drive.google.com (drive.google.com)... 74.125.140.101, 74.125.140.113, 74.125.140.139, ...\n","Connecting to drive.google.com (drive.google.com)|74.125.140.101|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pnoqcacftr2vnbq72v3g8se50sr6mb48/1612277550000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n","Warning: wildcards not supported in HTTP.\n","--2021-02-02 14:53:18--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pnoqcacftr2vnbq72v3g8se50sr6mb48/1612277550000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n","Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 173.194.76.132, 2a00:1450:400c:c00::84\n","Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|173.194.76.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-font-ttf]\n","Saving to: ‘taipei_sans_tc_beta.ttf’\n","\n","taipei_sans_tc_beta     [ <=>                ]  19.70M   114MB/s    in 0.2s    \n","\n","2021-02-02 14:53:18 (114 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qib-OcCLIkup"},"source":["## 機器翻譯訓練資料\r\n","來源: https://www.manythings.org/anki/\r\n","\r\n","Chinese (Mandarin) - English --> cmn-eng.zip (24360)\r\n","\r\n","解壓檔案並放置於 data 目錄"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1GC9mHUJByB","executionInfo":{"status":"ok","timestamp":1612277600525,"user_tz":-480,"elapsed":13642,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"96fcca78-a9ad-43b7-aba1-da0f369f6df2"},"source":["!wget https://www.manythings.org/anki/cmn-eng.zip\r\n","!unzip cmn-eng.zip -d data"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-02-02 14:53:19--  https://www.manythings.org/anki/cmn-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1062383 (1.0M) [application/zip]\n","Saving to: ‘cmn-eng.zip’\n","\n","cmn-eng.zip         100%[===================>]   1.01M  1.30MB/s    in 0.8s    \n","\n","2021-02-02 14:53:20 (1.30 MB/s) - ‘cmn-eng.zip’ saved [1062383/1062383]\n","\n","Archive:  cmn-eng.zip\n","  inflating: data/cmn.txt            \n","  inflating: data/_about.txt         \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFjVGqegR5U8","executionInfo":{"status":"ok","timestamp":1612277600525,"user_tz":-480,"elapsed":13635,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"f35c187c-270f-40ea-b77f-a5a063ed5432"},"source":["data_dir = 'data'\n","with open(os.path.join(data_dir, 'cmn.txt'), encoding='utf-8') as f:\n","    lines = f.read().strip().split('\\n')\n","    trnslt_pairs = [l.split('\\t') for l in lines]\n","\n","print(f\"Sample: {trnslt_pairs[1000][0:2]}\")\n","print(f\"Total records: {len(trnslt_pairs)}\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Sample: ['He was drowned.', '他被淹死了。']\n","Total records: 24360\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iL_qV-iBK1jn"},"source":["## 將資料切分成 train/valiation/test 三個資料集，且以 csv 格式儲存在 data 目錄"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gnp47zRCK0gQ","executionInfo":{"status":"ok","timestamp":1612277600526,"user_tz":-480,"elapsed":13629,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"b4a7ce47-d8fe-46da-c3eb-6b4998c672d4"},"source":["train, test = train_test_split(trnslt_pairs, test_size=0.1)\r\n","train, val = train_test_split(train, test_size=0.1)\r\n","print(f\"training data: {len(train)} , validation data: {len(val)} , testing data: {len(test)}\")\r\n","\r\n","def write_csv(data, file_path):\r\n","    with open(file_path, 'w', newline='', encoding='utf-8') as f:\r\n","        writer = csv.writer(f)\r\n","        for item in data:\r\n","            writer.writerow([item[0], item[1]])\r\n","\r\n","write_csv(train, os.path.join(data_dir, 'train.csv'))\r\n","write_csv(val, os.path.join(data_dir, 'val.csv'))\r\n","write_csv(test, os.path.join(data_dir, 'test.csv'))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["training data: 19731 , validation data: 2193 , testing data: 2436\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ba8ALJB4MWOe"},"source":["## 下載 spacy 英文/中文語料"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvnlDPENQEsO","executionInfo":{"status":"ok","timestamp":1612277616559,"user_tz":-480,"elapsed":29656,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"463898a0-046c-437d-f05c-d404e5691313"},"source":["import spacy.cli\r\n","spacy.cli.download(\"en_core_web_sm\")\r\n","spacy.cli.download(\"zh_core_web_sm\")\r\n","\r\n","import en_core_web_sm, zh_core_web_sm\r\n","spacy_en, spacy_zh = en_core_web_sm.load(), zh_core_web_sm.load()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('zh_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VTG14b5wueNs"},"source":["## 使用 torchtext 來準備訓練資料"]},{"cell_type":"code","metadata":{"id":"okavgkx0uk3y","executionInfo":{"status":"ok","timestamp":1612277616561,"user_tz":-480,"elapsed":29657,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def tokenize_zh(text):\r\n","    # 移除非中文字元\r\n","    text = re.sub(r'[^\\u4e00-\\u9fa5A-Za-z0-9]', '', text)\r\n","\r\n","    return [word.text for word in spacy_zh.tokenizer(text)]\r\n","\r\n","def tokenize_en(text):\r\n","    # 清除不需要的字元\r\n","    text = re.sub(r'([.!?])', r' \\1', text)\r\n","\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n","\r\n","TRG = Field(tokenize=tokenize_en, init_token='<sos>',\r\n","            eos_token='<eos>', lower=True)\r\n","SRC = Field(tokenize=tokenize_zh, init_token='<sos>', eos_token='<eos>',\r\n","            lower=True, include_lengths=True)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwkGkK6tv295"},"source":["## 使用 TabularDataset 讀資料並建立辭典"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHJSLLihv4Cp","executionInfo":{"status":"ok","timestamp":1612277629085,"user_tz":-480,"elapsed":42174,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"7895b8e4-b4a8-4481-b6d3-c404ae7d1e39"},"source":["train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\r\n","    path=data_dir, format='csv', skip_header=True,\r\n","    train='train.csv', validation='val.csv', test='test.csv',\r\n","    fields=[('trg', TRG), ('src', SRC)]\r\n",")\r\n","SRC.build_vocab(train_dataset, min_freq=1)\r\n","TRG.build_vocab(train_dataset, min_freq=1)\r\n","print(f\"中文語料辭典大小: {len(SRC.vocab)}, 英文辭典大小: {len(TRG.vocab)}\")\r\n","print(f\"Sample SRC: {test_dataset[0].src}, TRG: {test_dataset[0].trg}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["中文語料辭典大小: 12282, 英文辭典大小: 6115\n","Sample SRC: ['翻译', '这个', '句子', '会', '很', '容易'], TRG: ['translating', 'this', 'sentence', 'will', 'be', 'very', 'easy', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tl-KIM-nSA-H","executionInfo":{"status":"ok","timestamp":1612277629086,"user_tz":-480,"elapsed":42173,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["BATCH_SIZE = 128\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset, dev_dataset, test_dataset),\n","    batch_size=BATCH_SIZE,\n","    sort_within_batch=True,\n","    sort_key=lambda x: len(x.src),\n","    device=device\n",")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYoqlKcrq2Z_"},"source":["## Attention 層(向量拼接方式)"]},{"cell_type":"code","metadata":{"id":"wj3ZTHDMSGOF","executionInfo":{"status":"ok","timestamp":1612277629086,"user_tz":-480,"elapsed":42172,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["class Attention(nn.Module):\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\n","        super(Attention, self).__init__()\n","        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n","        \n","    def forward(self, hidden, encoder_outputs, mask):\n","        '''\n","            hidden: [batch_size, dec_hid_dim]\n","            encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n","            mask: [batch_size, src_len]\n","        '''\n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","        \n","        # (batch_size, src_len, dec_hid_dim)\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        # (batch_size, src_len, enc_hid_dim * 2)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        # (batch_size, src_len, dec_hid_dim)\n","        attention = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2))) \n","        attention = self.v(attention).squeeze(2)  # (batch_size, src_len)\n","        \n","        attention = attention.masked_fill(mask==0, -1e10)\n","        \n","        return F.softmax(attention, dim=1)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V5Svj0D91vrq"},"source":["## Bi-directional Encoder(GRU)"]},{"cell_type":"code","metadata":{"id":"L8AVtNly1opU","executionInfo":{"status":"ok","timestamp":1612277629087,"user_tz":-480,"elapsed":42171,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["class RNNEncoder(nn.Module):\r\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout_rate):\r\n","        super(RNNEncoder, self).__init__()\r\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n","\r\n","        # bi-directional GRU encoder\r\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\r\n","        \r\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n","        self.dropout = nn.Dropout(dropout_rate)\r\n","        \r\n","    def forward(self, src, src_len):\r\n","        '''\r\n","        Args:\r\n","            src: [src_len, batch_size]\r\n","            src_len: [batch_size]\r\n","        Returns:\r\n","            outputs: [src_len, batch_size, enc_hid_dim * 2]\r\n","            hidden: [batch_size, dec_hid_dim]\r\n","        '''\r\n","        # (src_len, batch_size, emb_dim)\r\n","        embedded = self.dropout(self.embedding(src))\r\n","                \r\n","        packed_embedded = pack_padded_sequence(embedded, src_len)\r\n","        # hidden: [n_layers * num_directions, batch_size, hid_dim]\r\n","        packed_outputs, hidden = self.rnn(packed_embedded)\r\n","        outputs, _ = pad_packed_sequence(packed_outputs) \r\n","        # (src_len, batch_size, hid_dim * num_directions)\r\n","        \r\n","        # hidden[-2, :, : ] 最後一層 forwards RNN\r\n","        # hidden[-1, :, : ] 最後一層 backwards RNN\r\n","        hidden = torch.tanh(\r\n","            self.fc(self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\r\n","        )\r\n","\r\n","        return outputs, hidden"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVVFtNZv1-zA"},"source":["## RNN Decoder(GRU)"]},{"cell_type":"code","metadata":{"id":"P-W2XwE51_IF","executionInfo":{"status":"ok","timestamp":1612277629087,"user_tz":-480,"elapsed":42170,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["class RNNDecoder(nn.Module):\r\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout_rate, attention):\r\n","        super(RNNDecoder, self).__init__()\r\n","        self.output_dim = output_dim\r\n","\r\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n","        self.attention = attention\r\n","        \r\n","        # GRU decoder\r\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n","        \r\n","        self.fc_out = nn.Linear(\r\n","            (enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim\r\n","        )\r\n","        self.dropout = nn.Dropout(dropout_rate)\r\n","        \r\n","    def forward(self, input, hidden, encoder_outputs, mask):\r\n","        '''\r\n","            input: [batch_size]\r\n","            hidden: [batch_size, dec_hid_dim]\r\n","            encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\r\n","            mask: [batch_size, src_len]\r\n","        '''\r\n","        input = input.unsqueeze(0)  # (1, batch_size)\r\n","        # (1, batch_size, emb_dim)\r\n","        embedded = self.dropout(self.embedding(input))\r\n","        \r\n","        # (batch_size, src_len)\r\n","        att = self.attention(hidden, encoder_outputs, mask)\r\n","        att = att.unsqueeze(1)  # (batch_size, 1, src_len)\r\n","\r\n","        # (batch_size, src_len, enc_hid_dim * 2)\r\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n","        # (batch_size, 1, enc_hid_dim * 2)\r\n","        weighted = torch.bmm(att, encoder_outputs)\r\n","        weighted = weighted.permute(1, 0, 2)  # (1, batch_size, enc_hid_dim * 2)\r\n","        \r\n","        # (1, batch_size, (enc_hid_dim * 2) + emb_dim)\r\n","        rnn_input = torch.cat((embedded, weighted), dim=2)\r\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n","        # output: [seq_len, batch_size, dec_hid_dim * n_directions]\r\n","        # hidden: [n_layers * n_directions, batch_size, dec_hid_dim]\r\n","        \r\n","        # seq_len, n_layers and n_directions will always be 1 in this decoder, therefore:\r\n","        # output: [1, batch_size, dec_hid_dim]\r\n","        # hidden: [1, batch_size, dec_hid_dim]\r\n","        # this also means that output == hidden\r\n","        assert (output == hidden).all()\r\n","        \r\n","        embedded = embedded.squeeze(0)\r\n","        output = output.squeeze(0)\r\n","        weighted = weighted.squeeze(0)\r\n","        \r\n","        # (batch_size, output_dim)\r\n","        prediction = self.fc_out(self.dropout(torch.cat((output, weighted, embedded), dim=1)))\r\n","                \r\n","        return prediction, hidden.squeeze(0), att.squeeze(1)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiytVpfk4isG"},"source":["## Seq2Seq 模組"]},{"cell_type":"code","metadata":{"id":"TNzgZqHcS2CX","executionInfo":{"status":"ok","timestamp":1612277629087,"user_tz":-480,"elapsed":42169,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["class Seq2SeqATTN(nn.Module):\n","    def __init__(self, encoder, decoder, src_pad_idx, device):\n","        super(Seq2SeqATTN, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.device = device\n","        \n","    def create_mask(self, src):\n","        mask = (src != self.src_pad_idx).permute(1, 0)\n","\n","        return mask\n","        \n","    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n","        '''\n","            src: [src_len, batch_size]\n","            src_len: [batch_size]\n","            trg: [trg_len, batch_size]\n","            teacher_forcing_ratio is probability to use teacher forcing\n","            e.g. if teacher_forcing_ratio is 0.75, we use teacher forcing 75% of the time\n","        '''\n","        batch_size = src.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        # encoder_outputs is all hidden states of the input sequence, back and forwards\n","        # hidden is the final forward and backward hidden states, passed through a linear layer\n","        encoder_outputs, hidden = self.encoder(src, src_len)\n","                \n","        # first input to the decoder is the <sos> tokens\n","        input = trg[0,:]\n","        mask = self.create_mask(src)  # (batch_size, src_len)\n","\n","        for t in range(1, trg_len):\n","            # insert input token embedding, previous hidden state, all encoder hidden states and mask\n","            # receive output tensor (predictions) and new hidden state\n","            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n","            \n","            # place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","            \n","            # decide if we are going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            # get the highest predicted token from our predictions\n","            top1 = output.argmax(1) \n","            \n","            # if teacher forcing, use actual next token as next input\n","            # if not, use predicted token\n","            input = trg[t] if teacher_force else top1\n","            \n","        return outputs"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pW2KIxhxrMGf"},"source":["## 建立模型和重要參數"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybIY0kKGS_gI","executionInfo":{"status":"ok","timestamp":1612277638027,"user_tz":-480,"elapsed":51103,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"b8eb9667-79f4-479c-c84a-4be4a4a89246"},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 256  # 注意 encoder hidden dim 設定必須為 decoder 的一半 \n","DEC_HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","LEARNING_RATE = 0.001\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n","\n","def initial_mdl_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","model.apply(initial_mdl_weights)\n","print(f\"模型全部參數量: {sum(p.numel() for p in model.parameters()):10,d}\")\n","model"],"execution_count":14,"outputs":[{"output_type":"stream","text":["模型全部參數量: 16,089,571\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Seq2SeqATTN(\n","  (encoder): RNNEncoder(\n","    (embedding): Embedding(12282, 256)\n","    (rnn): GRU(256, 256, bidirectional=True)\n","    (fc): Linear(in_features=512, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): RNNDecoder(\n","    (embedding): Embedding(6115, 256)\n","    (attention): Attention(\n","      (attn): Linear(in_features=1024, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (rnn): GRU(768, 512)\n","    (fc_out): Linear(in_features=1280, out_features=6115, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"PrjwJTlB70h7"},"source":["## train and evalutate functions"]},{"cell_type":"code","metadata":{"id":"KOpjxQJmTDYU","executionInfo":{"status":"ok","timestamp":1612277638027,"user_tz":-480,"elapsed":51101,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    model.train()\n","    \n","    epoch_loss = 0\n","    for batch in iterator:\n","        src, src_len = batch.src\n","        trg = batch.trg  # (trg_len, batch_size)\n","        \n","        # (trg_len, batch_size, output_dim)\n","        output = model(src, src_len.cpu() , trg)\n","        output_dim = output.shape[-1]\n","        \n","        # ((trg_len - 1) * batch_size, output_dim)\n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)  # ((trg_len - 1) * batch_size)\n","        \n","        loss = criterion(output, trg)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    model.eval()\n","    \n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in iterator:\n","            src, src_len = batch.src\n","            trg = batch.trg  # (trg_len, batch_size)\n","\n","            # (trg_len, batch_size, output_dim)\n","            output = model(src, src_len.cpu(), trg, 0)  # turn off teacher forcing\n","            output_dim = output.shape[-1]\n","\n","            # ((trg len - 1) * batch_size, output_dim)\n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)  # ((trg len - 1) * batch_size)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AFZUwxJA8CF6"},"source":["## 訓練設定"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcmz65KO8FF1","executionInfo":{"status":"ok","timestamp":1612278471388,"user_tz":-480,"elapsed":884456,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"1919273f-744c-4182-8049-4475cf820c1b"},"source":["MAX_EPOCHS = 25\r\n","CLIP = 5\r\n","model_dir = 'model'\r\n","os.makedirs(model_dir, exist_ok=True)\r\n","\r\n","best_valid_loss = 9999999\r\n","for epoch in range(MAX_EPOCHS):\r\n","    start_time = time.time()\r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    end_time = time.time()\r\n","    \r\n","    torch.save(model.state_dict(), os.path.join(model_dir, f\"model-{epoch}.pt\"))\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), os.path.join(model_dir, 'best-model.pt'))\r\n","   \r\n","    print(f\"Epoch {epoch:2d} training time: {end_time - start_time:.2f} sec, \"\r\n","          f\"Training Loss: {train_loss:.3f}, Valiation Loss: {valid_loss:.3f}\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch  0 training time: 33.15 sec, Training Loss: 5.500, Valiation Loss: 5.103\n","Epoch  1 training time: 33.19 sec, Training Loss: 4.709, Valiation Loss: 4.839\n","Epoch  2 training time: 33.19 sec, Training Loss: 4.329, Valiation Loss: 4.719\n","Epoch  3 training time: 33.19 sec, Training Loss: 4.064, Valiation Loss: 4.413\n","Epoch  4 training time: 33.48 sec, Training Loss: 3.746, Valiation Loss: 4.244\n","Epoch  5 training time: 33.12 sec, Training Loss: 3.478, Valiation Loss: 4.111\n","Epoch  6 training time: 33.44 sec, Training Loss: 3.193, Valiation Loss: 3.966\n","Epoch  7 training time: 33.12 sec, Training Loss: 2.946, Valiation Loss: 3.868\n","Epoch  8 training time: 33.15 sec, Training Loss: 2.742, Valiation Loss: 3.825\n","Epoch  9 training time: 33.13 sec, Training Loss: 2.503, Valiation Loss: 3.797\n","Epoch 10 training time: 32.89 sec, Training Loss: 2.319, Valiation Loss: 3.703\n","Epoch 11 training time: 32.94 sec, Training Loss: 2.141, Valiation Loss: 3.700\n","Epoch 12 training time: 33.04 sec, Training Loss: 1.991, Valiation Loss: 3.700\n","Epoch 13 training time: 33.06 sec, Training Loss: 1.888, Valiation Loss: 3.684\n","Epoch 14 training time: 33.14 sec, Training Loss: 1.778, Valiation Loss: 3.644\n","Epoch 15 training time: 33.00 sec, Training Loss: 1.667, Valiation Loss: 3.665\n","Epoch 16 training time: 33.14 sec, Training Loss: 1.571, Valiation Loss: 3.717\n","Epoch 17 training time: 32.96 sec, Training Loss: 1.476, Valiation Loss: 3.716\n","Epoch 18 training time: 32.79 sec, Training Loss: 1.405, Valiation Loss: 3.722\n","Epoch 19 training time: 32.84 sec, Training Loss: 1.361, Valiation Loss: 3.781\n","Epoch 20 training time: 32.78 sec, Training Loss: 1.307, Valiation Loss: 3.742\n","Epoch 21 training time: 32.95 sec, Training Loss: 1.250, Valiation Loss: 3.763\n","Epoch 22 training time: 33.11 sec, Training Loss: 1.174, Valiation Loss: 3.797\n","Epoch 23 training time: 32.83 sec, Training Loss: 1.178, Valiation Loss: 3.812\n","Epoch 24 training time: 32.88 sec, Training Loss: 1.119, Valiation Loss: 3.849\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t9F-sWnV9f28"},"source":["## Evaluate testing set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ukg9t_iOTHlG","executionInfo":{"status":"ok","timestamp":1612278472510,"user_tz":-480,"elapsed":885573,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"cfec7b1e-9b10-4099-b502-d0f3fe49719a"},"source":["model.load_state_dict(torch.load(os.path.join(model_dir, 'best-model.pt')))\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["| Test Loss: 3.754 | Test PPL:  42.675 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q-caE1Y1TL5p","executionInfo":{"status":"ok","timestamp":1612278472510,"user_tz":-480,"elapsed":885571,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n","    model.eval()\n","        \n","    tokens = [token.lower() for token in sentence]\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n","    \n","    with torch.no_grad():\n","        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n","\n","    mask = model.create_mask(src_tensor)\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n","    \n","    for i in range(max_len):\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n","\n","        attentions[i] = attention\n","\n","        pred_token = output.argmax(1).item()\n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","    \n","    return trg_tokens[1:], attentions[:len(trg_tokens) - 1]"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7JxlvTVD-kvT"},"source":["## 選擇一句來看翻譯結果"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b22YJ_44-l_O","executionInfo":{"status":"ok","timestamp":1612278472511,"user_tz":-480,"elapsed":885565,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"5c39d8ea-7fc0-417d-ecbf-05643e469ebc"},"source":["example_idx = 123\r\n","\r\n","src = vars(train_dataset.examples[example_idx])['src']\r\n","print(f'src = {src}')\r\n","trg = vars(train_dataset.examples[example_idx])['trg']\r\n","print(f'trg = {trg}')\r\n","\r\n","translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n","print(f'predicted trg = {translation}')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["src = ['這位', '老人', '餓死', '了']\n","trg = ['the', 'old', 'man', 'starved', 'to', 'death', '.']\n","predicted trg = ['the', 'old', 'man', 'starved', 'to', '.', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EWpOXdiL_c9A"},"source":["## BLEU score evaluation"]},{"cell_type":"code","metadata":{"id":"vgHrm83K_dNX","executionInfo":{"status":"ok","timestamp":1612278472511,"user_tz":-480,"elapsed":885564,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def calculate_bleu(data, src_field, trg_field, model, device, max_len=50):    \r\n","    trgs, pred_trgs = [], []\r\n","    for datum in data:\r\n","        src = vars(datum)['src']\r\n","        trg = vars(datum)['trg']\r\n","        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\r\n","        \r\n","        # cut off <eos> token\r\n","        pred_trg = pred_trg[:-1]\r\n","        \r\n","        pred_trgs.append(pred_trg)\r\n","        trgs.append([trg])\r\n","        \r\n","    return bleu_score(pred_trgs, trgs)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUvCDups_mdB","executionInfo":{"status":"ok","timestamp":1612278509703,"user_tz":-480,"elapsed":922750,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"29c8ea20-8410-4564-e758-415861eebb47"},"source":["bleu_score = calculate_bleu(test_dataset, SRC, TRG, model, device)\r\n","print(f'BLEU score = {bleu_score * 100:.2f}')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["BLEU score = 17.75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9jhh_5_SYYLT","executionInfo":{"status":"ok","timestamp":1612278509704,"user_tz":-480,"elapsed":922750,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def display_attention(sentence, translation, attention):\n","    src = ['<sos>'] + [t.lower() for t in sentence] + ['<eos>']\n","    trg = translation\n","    att = attention.squeeze(1).cpu().detach().numpy()\n","\n","    df = pd.DataFrame(data=att, index=trg, columns=src)\n","    fig, ax = plt.subplots(figsize=(8, 8))\n","\n","    plt.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n","    sns.heatmap(df, vmin=0, vmax=1.0, ax=ax, cmap=\"Blues\", cbar=False)\n","    label_y = ax.get_yticklabels()\n","    plt.setp(label_y, rotation=360, ha='right', fontproperties=myfont)\n","    label_x = ax.get_xticklabels()\n","    plt.setp(label_x, rotation=90, ha='right', fontproperties=myfont)\n","    plt.show()"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4n7915Mrcs1"},"source":["## 作業重點\n","請選擇一個好的翻譯結果並將其 Attention 視覺化"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pRYXjqgvYb-E","executionInfo":{"status":"ok","timestamp":1612278509705,"user_tz":-480,"elapsed":922745,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"ae20392c-807e-4181-91f6-8935029b6631"},"source":["# 請在這自行調整 sample index 觀察不同句子的 Attention 結果\n","example_idx = 456\n","\n","src = vars(train_dataset.examples[example_idx])['src']\n","print(f'src = {src}')\n","trg = vars(train_dataset.examples[example_idx])['trg']\n","print(f'trg = {trg}')\n","\n","translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","print(f'predicted trg = {translation}')"],"execution_count":23,"outputs":[{"output_type":"stream","text":["src = ['喂', '你', '能', '把', '灯', '关掉', '吗']\n","trg = ['hey', ',', 'can', 'you', 'turn', 'the', 'lights', 'off', '?']\n","predicted trg = ['can', 'you', 'drive', 'you', 'turn', 'on', 'the', 'lights', '?', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2cdlptGJrsfv"},"source":["# 觀察翻譯文和被翻譯文的語意對應"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"H8f8csPSYfkU","executionInfo":{"status":"ok","timestamp":1612278510224,"user_tz":-480,"elapsed":923258,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"8d24cf36-4e0f-4f25-f352-b224393b2d00"},"source":["print(''.join(src))\n","display_attention(src, translation, attention)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["喂你能把灯关掉吗\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}