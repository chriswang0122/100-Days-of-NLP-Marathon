{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hw_IMDB_LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q0y1IFdUh5zs"},"source":["# 文本情感分析：使用 LSTM\n","\n","我們將使用 LSTM 方法來做情感(情緒)分析，資料集是著名的電影評論資料集 IMDB dataset.\n","期望達到不錯的準確度 84% 以上\n"]},{"cell_type":"markdown","metadata":{"id":"WtKPzAnqh5zs"},"source":["## 準備資料\n","\n","torchtext 包含以下 components：\n","\n","Field : 主要包含以下數據預處理的配置訊息：指定分詞方法、是否轉成小寫、起始符號、以及字典等等。\n","\n","Dataset : 用於下載數據，也提供 splits 方法可以同時下載訓練資料、驗證資料和測試資料。\n","\n","Iterator : 數據讀取的迭代器，可以支持 batch\n","\n","我們定義 SEED、TEXT 和 LABEL 三個變數來隨機把資料集分割成 train/valid/test 三個資料集。\n","\n"]},{"cell_type":"code","metadata":{"id":"BpIzg9Ehh5zs","executionInfo":{"status":"ok","timestamp":1610867679531,"user_tz":-480,"elapsed":8227,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import torch\n","from torchtext import data, datasets\n","\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize='spacy', include_lengths=True)\n","LABEL = data.LabelField(dtype=torch.float)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5im_3jPuh5zs"},"source":["## 下載並讀取資料\n","\n","torchtext 的 datasets 集合裡面就有 IMDB 資料，可以直接讀取訓練以及測試資料。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQBm1Pihh5zs","executionInfo":{"status":"ok","timestamp":1610867778198,"user_tz":-480,"elapsed":106857,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"35a89bdb-b58d-4b98-9d92-a51f69d6adc6"},"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:04<00:00, 20.9MB/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lg3dDAnvh5zs"},"source":["\n","## 從訓練資料裡面切割驗證資料\n","\n","從訓練資料裡面抓取一些資料當作 validation set"]},{"cell_type":"code","metadata":{"id":"LJ3m5a4jh5zs","executionInfo":{"status":"ok","timestamp":1610867778199,"user_tz":-480,"elapsed":106856,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import random\n","\n","train_data, valid_data = train_data.split(random_state=random.seed(SEED))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rt_o8fRh5zs"},"source":["# 建立字典\n","\n","接下來是使用預訓練的 word embeddings。只要呼叫 TorchText 的 build_vocab 就可以把所有的文字向量化, 我們使用的是 \"glove.6B.100d\" 的向量，glove 是一種用來計算詞向量的演算法。6B 是指這些詞向量用了60億個tokens訓練出來的，而 100d 是指每一個向量的維度是 100。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y855ENTLh5zs","executionInfo":{"status":"ok","timestamp":1610868216084,"user_tz":-480,"elapsed":544734,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"52f4b432-c1bf-45e7-b569-aa3da373a1ac"},"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size=MAX_VOCAB_SIZE, \n","                 vectors=\"glove.6B.100d\",\n","                 unk_init=torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":4,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n","100%|█████████▉| 398250/400000 [00:16<00:00, 24788.41it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"KTJfJYW_h5zs"},"source":["#建立Iterator\n","\n","Iterator 是 torchtext 到模型的輸出，提供對數據的打亂、排序等等處理方法。可以動態修改 batch size這裡使用 splits method 來同時輸出訓練集、驗證集以及測試集。\n","\n","如果有 GPU 的話則使用 cuda 來做運算。\n","\n","`sort_within_batch = True` 是表示在每一個 batch 裡面的 tensors 是依照長度排序的。"]},{"cell_type":"code","metadata":{"id":"KqCebFvAh5zs","executionInfo":{"status":"ok","timestamp":1610868216088,"user_tz":-480,"elapsed":544734,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size=BATCH_SIZE,\n","    sort_within_batch=True,\n","    device=device\n",")"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGpIKauzh5zs"},"source":["# 建立 LSTM 模型\n","我們將使用 PyTorch 內建的 RNN 架構 LSTM (Long Short-Term Memory)模型。它的公式如下：\n","\n","$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$\n","\n","\n","步驟解釋：\n","\n","1. 在模型裡，每個詞會先通過 embedding layer 得到特徵向量\n","2. 然後使用 LSTM 對特徵序列進一步編碼得到序列信息\n","3. 將編碼後的序列信息通過全連接層(Fully connectivity layer)得到輸出\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"hjCYAR_Eh5zs","executionInfo":{"status":"ok","timestamp":1610868216088,"user_tz":-480,"elapsed":544731,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim,\n","                           num_layers=n_layers,\n","                           bidirectional=bidirectional,\n","                           dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text, text_lengths):\n","        # text = (sent_len, batch_size)\n","        embedded = self.dropout(self.embedding(text))  # (sent_len, batch_size, emb_dim)\n","        \n","        # pack sequence\n","        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu())\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        # unpack sequence\n","        output, output_lengths = pad_packed_sequence(packed_output)\n","        # output = (sent_len, batch_size, hid_dim * num_directions)\n","        # output over padding tokens are zero tensors\n","        \n","        # hidden = (num_layers * num_directions, batch_size, hid_dim)\n","        # cell = (num_layers * num_directions, batch_size, hid_dim)\n","        \n","        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers and apply dropout\n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n","        # hidden = (batch_size, hid_dim * num_directions)\n","            \n","        return self.fc(hidden)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4s7IySNeh5zs"},"source":["\n","# LSTM 模型參數說明\n","\n","1. vocab_size: 輸入層的維度(input dim)\n","2. embedding_dim: 詞向量的維度, 我們使用的是 glove.6B.100d, 所以這裡 embedding_dim 是 100\n","3. hidden_dim: the size of the hidden states\n","4. output_dim: 輸出層的維度\n","5. n_layers: 有幾層RNN層\n","6. bidirectional: 是否使用雙向 RNN\n","7. dropout： dropout 的比例\n","8. pad_idx: token <pad> 的 index\n"]},{"cell_type":"code","metadata":{"id":"0nRixhDvh5zs","executionInfo":{"status":"ok","timestamp":1610868216089,"user_tz":-480,"elapsed":544729,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = LSTM(INPUT_DIM, \n","             EMBEDDING_DIM, \n","             HIDDEN_DIM, \n","             OUTPUT_DIM, \n","             N_LAYERS, \n","             BIDIRECTIONAL, \n","             DROPOUT, \n","             PAD_IDX)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpYQJmGih5zs"},"source":["印出模型的參數量"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"clRTL5Hih5zs","executionInfo":{"status":"ok","timestamp":1610868216089,"user_tz":-480,"elapsed":544723,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"706c90da-1734-42db-fd04-f60aadede68b"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model)} trainable parameters')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The model has 4810857 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iBc7QkGjh5zt"},"source":["\n","檢查 embedding 的字典大小以及 embedding 的維度\n","\n","**(vocab_size, embedding_dim)**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1k7sY2aCh5zt","executionInfo":{"status":"ok","timestamp":1610868216089,"user_tz":-480,"elapsed":544717,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"a2ba0dc5-805d-4487-c08b-5b3ac0eeacc3"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","print(pretrained_embeddings.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j2BPDdj-h5zt"},"source":["用 pre-trained embeddings 來當作 `embedding` 層的初始化參數\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUZmehuKh5zt","executionInfo":{"status":"ok","timestamp":1610868216450,"user_tz":-480,"elapsed":545072,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"ffbcf7d2-5516-42ce-88cd-54fe4434e54b"},"source":["model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n","        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.1532,  0.0293,  0.2073,  ...,  0.1447,  0.1620,  0.8675],\n","        [-0.8353, -0.9495, -0.2011,  ..., -0.2850, -0.3028, -0.2606],\n","        [ 0.8068,  0.7625, -0.0371,  ..., -0.5529,  0.2119, -0.1345]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"yAO6zCL9h5zt"},"source":["因為`<unk>` and `<pad>`是沒有在 pre-trained 的詞裡面，所以要把 `<unk>` 和 `<pad>`的 初始 embedding 權重都變成 0"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dy8PyFGLh5zu","executionInfo":{"status":"ok","timestamp":1610868216450,"user_tz":-480,"elapsed":545066,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"8b0941cb-4784-4035-da16-ba947282d0fd"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.1532,  0.0293,  0.2073,  ...,  0.1447,  0.1620,  0.8675],\n","        [-0.8353, -0.9495, -0.2011,  ..., -0.2850, -0.3028, -0.2606],\n","        [ 0.8068,  0.7625, -0.0371,  ..., -0.5529,  0.2119, -0.1345]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mbublLfuh5zu"},"source":["We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."]},{"cell_type":"markdown","metadata":{"id":"mQ3KLo-Bh5zu"},"source":["## 訓練模型"]},{"cell_type":"markdown","metadata":{"id":"ZOyVlzOgh5zu"},"source":["使用優化器 Adam"]},{"cell_type":"code","metadata":{"id":"z4pq14aXh5zu","executionInfo":{"status":"ok","timestamp":1610868216451,"user_tz":-480,"elapsed":545065,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJ-Obpa0h5zu"},"source":["使用 BCEWithLogitsLoss 當作 Loss Function"]},{"cell_type":"code","metadata":{"id":"mTqpIgdmh5zu","executionInfo":{"status":"ok","timestamp":1610868227087,"user_tz":-480,"elapsed":555699,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aj_qyWU1h5zu"},"source":["實作計算計算準確度的函式\n","\n"]},{"cell_type":"code","metadata":{"id":"bWMRD9Yfh5zu","executionInfo":{"status":"ok","timestamp":1610868227089,"user_tz":-480,"elapsed":555695,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def binary_accuracy(preds, y):\n","    '''\n","        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    '''\n","\n","    # round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float()  # convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QF66HLSsh5zu"},"source":["# 訓練函式"]},{"cell_type":"code","metadata":{"id":"nYmpftoIh5zu","executionInfo":{"status":"ok","timestamp":1610868227089,"user_tz":-480,"elapsed":555694,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    for batch in iterator:\n","        text, text_lengths = batch.text\n","        predictions = model(text, text_lengths).squeeze(1)\n","        acc = binary_accuracy(predictions, batch.label)\n","        epoch_acc += acc.item()\n","        \n","        loss = criterion(predictions, batch.label)\n","        epoch_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZmY5b9hh5zu"},"source":["# 測試模型的方法\n"]},{"cell_type":"code","metadata":{"id":"7i7SLjYoh5zu","executionInfo":{"status":"ok","timestamp":1610868227090,"user_tz":-480,"elapsed":555693,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["def evaluate(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","            text, text_lengths = batch.text\n","            predictions = model(text, text_lengths).squeeze(1)\n","\n","            acc = binary_accuracy(predictions, batch.label)\n","            epoch_acc += acc.item()\n","            \n","            loss = criterion(predictions, batch.label)\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1Ygz7V-h5zu"},"source":["查看訓練進度以及花費的時間"]},{"cell_type":"code","metadata":{"id":"3iCSdOs8h5zu","executionInfo":{"status":"ok","timestamp":1610868227090,"user_tz":-480,"elapsed":555691,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","\n","    return elapsed_mins, elapsed_secs"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bi8X4DTgh5zu"},"source":["開始訓練"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uav7h10Nh5zu","executionInfo":{"status":"ok","timestamp":1610868428378,"user_tz":-480,"elapsed":756972,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"03cfadbf-bd2f-4088-e14a-1199d145161b"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f\"Epoch: {epoch + 1:02d} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%\")\n","    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 398250/400000 [00:30<00:00, 24788.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 38s\n","\tTrain Loss: 0.679 | Train Acc: 58.02%\n","\t Val. Loss: 0.613 |  Val. Acc: 68.32%\n","Epoch: 02 | Epoch Time: 0m 39s\n","\tTrain Loss: 0.569 | Train Acc: 71.08%\n","\t Val. Loss: 0.443 |  Val. Acc: 80.39%\n","Epoch: 03 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.527 | Train Acc: 74.01%\n","\t Val. Loss: 0.392 |  Val. Acc: 83.02%\n","Epoch: 04 | Epoch Time: 0m 40s\n","\tTrain Loss: 0.460 | Train Acc: 78.65%\n","\t Val. Loss: 0.424 |  Val. Acc: 80.82%\n","Epoch: 05 | Epoch Time: 0m 40s\n","\tTrain Loss: 0.340 | Train Acc: 85.66%\n","\t Val. Loss: 0.306 |  Val. Acc: 87.35%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Xllf06gh5zu"},"source":["測試模型的準確度"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zrPaGTBh5zu","executionInfo":{"status":"ok","timestamp":1610868443699,"user_tz":-480,"elapsed":772287,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"ad1db56a-df37-4398-b1ad-c29cb7e5192f"},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Test Loss: 0.322 | Test Acc: 86.24%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lo2tHWvLh5zu"},"source":["## Demo 函式\n","\n","最後我們來建立一個 Demo 的函式讓使用者可以輸入任意的句子觀察模型是否可以正確的做好情緒分類。"]},{"cell_type":"code","metadata":{"id":"N1dQ2jL6h5zu","executionInfo":{"status":"ok","timestamp":1610868444972,"user_tz":-480,"elapsed":773558,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}}},"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor))\n","\n","    return prediction.item()"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNTmEPO2h5zu"},"source":["# Demo\n","\n","負面情緒的句子"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdyUrek4h5zu","executionInfo":{"status":"ok","timestamp":1610868444974,"user_tz":-480,"elapsed":773554,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"9fc82e84-59b0-4ed8-dbf9-d28c1dcd44b8"},"source":["predict_sentiment(model, 'This film is terrible')"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.009910310618579388"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"mua1ydUph5zu"},"source":["\n","正面情緒的句子"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0lAqqmYh5zu","executionInfo":{"status":"ok","timestamp":1610868444974,"user_tz":-480,"elapsed":773548,"user":{"displayName":"王俊煒","photoUrl":"","userId":"10865038187423164500"}},"outputId":"e182766b-a3f1-4f96-fe41-316bd7eebe36"},"source":["predict_sentiment(model, 'This film is great')"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9351367950439453"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"ByB6QdaFh5zu"},"source":["## 參考資料\n","- [torchtext 入門](https://zhuanlan.zhihu.com/p/31139113)\n","- [PyTorch Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)"]}]}