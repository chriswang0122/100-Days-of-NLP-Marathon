{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCIvz30AOj-H"
   },
   "source": [
    "# 作業 : 觀察機器翻譯 Attention 內容\n",
    "***\n",
    "## [作業目標]\n",
    "仔細地觀察機器翻譯 Attention 結果\n",
    "\n",
    "## [作業重點]\n",
    "透過視覺化注意力層瞭解 Attention 的運作方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8385,
     "status": "ok",
     "timestamp": 1612249364346,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "UIBD2Nn-OI-1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11810,
     "status": "ok",
     "timestamp": 1612249367783,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "FQoAR8K-RyHd",
    "outputId": "f73a9228-e49b-436d-b58e-cdb32e46d70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-02 07:02:44--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.164.174, 2607:f8b0:4004:815::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.164.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/76ies2f0iro71nl3io7j5dumagfvhhpj/1612249350000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-02-02 07:02:45--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/76ies2f0iro71nl3io7j5dumagfvhhpj/1612249350000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
      "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 172.217.12.225, 2607:f8b0:4004:807::2001\n",
      "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|172.217.12.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-font-ttf]\n",
      "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
      "\n",
      "taipei_sans_tc_beta     [  <=>               ]  19.70M  46.4MB/s    in 0.4s    \n",
      "\n",
      "2021-02-02 07:02:47 (46.4 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 進行 matplotlib 繪製中文\n",
    "# 下載字體\n",
    "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# 自定義字體\n",
    "myfont = FontProperties(fname='taipei_sans_tc_beta.ttf')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qib-OcCLIkup"
   },
   "source": [
    "## 機器翻譯訓練資料\r\n",
    "來源: https://www.manythings.org/anki/\r\n",
    "\r\n",
    "Chinese (Mandarin) - English --> cmn-eng.zip (24360)\r\n",
    "\r\n",
    "解壓檔案並放置於 data 目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12233,
     "status": "ok",
     "timestamp": 1612249368214,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "v1GC9mHUJByB",
    "outputId": "f60305e1-ee5e-4866-f468-b29989728031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-02 07:02:47--  https://www.manythings.org/anki/cmn-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3036::ac43:adc6, ...\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1062383 (1.0M) [application/zip]\n",
      "Saving to: ‘cmn-eng.zip’\n",
      "\n",
      "cmn-eng.zip         100%[===================>]   1.01M  4.11MB/s    in 0.2s    \n",
      "\n",
      "2021-02-02 07:02:47 (4.11 MB/s) - ‘cmn-eng.zip’ saved [1062383/1062383]\n",
      "\n",
      "Archive:  cmn-eng.zip\n",
      "  inflating: data/cmn.txt            \n",
      "  inflating: data/_about.txt         \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.manythings.org/anki/cmn-eng.zip\r\n",
    "!unzip cmn-eng.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12224,
     "status": "ok",
     "timestamp": 1612249368215,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "wFjVGqegR5U8",
    "outputId": "c1de7892-1e1b-4ccc-dec2-1884365189bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ['He was drowned.', '他被淹死了。']\n",
      "Total records: 24360\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "with open(os.path.join(data_dir, 'cmn.txt'), encoding='utf-8') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    trnslt_pairs = [l.split('\\t') for l in lines]\n",
    "\n",
    "print(f\"Sample: {trnslt_pairs[1000][0:2]}\")\n",
    "print(f\"Total records: {len(trnslt_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL_qV-iBK1jn"
   },
   "source": [
    "## 將資料切分成 train/valiation/test 三個資料集，且以 csv 格式儲存在 data 目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12218,
     "status": "ok",
     "timestamp": 1612249368215,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "Gnp47zRCK0gQ",
    "outputId": "d3226109-7d89-4b84-8f83-eac1b20d3361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 19731 , validation data: 2193 , testing data: 2436\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(trnslt_pairs, test_size=0.1)\r\n",
    "train, val = train_test_split(train, test_size=0.1)\r\n",
    "print(f\"training data: {len(train)} , validation data: {len(val)} , testing data: {len(test)}\")\r\n",
    "\r\n",
    "def write_csv(data, file_path):\r\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as f:\r\n",
    "        writer = csv.writer(f)\r\n",
    "        for item in data:\r\n",
    "            writer.writerow([item[0], item[1]])\r\n",
    "\r\n",
    "write_csv(train, os.path.join(data_dir, 'train.csv'))\r\n",
    "write_csv(val, os.path.join(data_dir, 'val.csv'))\r\n",
    "write_csv(test, os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba8ALJB4MWOe"
   },
   "source": [
    "## 下載 spacy 英文/中文語料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36129,
     "status": "ok",
     "timestamp": 1612249392131,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "PvnlDPENQEsO",
    "outputId": "9432c2ed-78e2-40ee-eed5-cdfa95fc28ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('zh_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\r\n",
    "spacy.cli.download(\"en_core_web_sm\")\r\n",
    "spacy.cli.download(\"zh_core_web_sm\")\r\n",
    "\r\n",
    "import en_core_web_sm, zh_core_web_sm\r\n",
    "spacy_en, spacy_zh = en_core_web_sm.load(), zh_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTG14b5wueNs"
   },
   "source": [
    "## 使用 torchtext 來準備訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 36125,
     "status": "ok",
     "timestamp": 1612249392132,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "okavgkx0uk3y"
   },
   "outputs": [],
   "source": [
    "def tokenize_zh(text):\r\n",
    "    # 移除非中文字元\r\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5A-Za-z0-9]', '', text)\r\n",
    "\r\n",
    "    return [word.text for word in spacy_zh.tokenizer(text)]\r\n",
    "\r\n",
    "def tokenize_en(text):\r\n",
    "    # 清除不需要的字元\r\n",
    "    text = re.sub(r'([.!?])', r' \\1', text)\r\n",
    "\r\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
    "\r\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>',\r\n",
    "            eos_token='<eos>', lower=True)\r\n",
    "SRC = Field(tokenize=tokenize_zh, init_token='<sos>', eos_token='<eos>',\r\n",
    "            lower=True, include_lengths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwkGkK6tv295"
   },
   "source": [
    "## 使用 TabularDataset 讀資料並建立辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45583,
     "status": "ok",
     "timestamp": 1612249401593,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "xHJSLLihv4Cp",
    "outputId": "9a6ec9b9-3c1b-4316-f7c2-18dd81c26ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文語料辭典大小: 12298, 英文辭典大小: 6078\n",
      "Sample SRC: ['我', '知道', '我', '們', '要', '怎麼', '幫', '忙'], TRG: ['i', 'know', 'how', 'we', 'can', 'help', '.']\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\r\n",
    "    path=data_dir, format='csv', skip_header=True,\r\n",
    "    train='train.csv', validation='val.csv', test='test.csv',\r\n",
    "    fields=[('trg', TRG), ('src', SRC)]\r\n",
    ")\r\n",
    "SRC.build_vocab(train_dataset, min_freq=1)\r\n",
    "TRG.build_vocab(train_dataset, min_freq=1)\r\n",
    "print(f\"中文語料辭典大小: {len(SRC.vocab)}, 英文辭典大小: {len(TRG.vocab)}\")\r\n",
    "print(f\"Sample SRC: {test_dataset[0].src}, TRG: {test_dataset[0].trg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 45578,
     "status": "ok",
     "timestamp": 1612249401594,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "Tl-KIM-nSA-H"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, dev_dataset, test_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYoqlKcrq2Z_"
   },
   "source": [
    "## Attention 層(點積)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 45568,
     "status": "ok",
     "timestamp": 1612249401594,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "wj3ZTHDMSGOF"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        '''\n",
    "            hidden: [batch_size, dec_hid_dim]\n",
    "            encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "            mask: [batch_size, src_len]\n",
    "        '''\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        hidden = hidden.unsqueeze(1)  # (batch_size, 1 dec_hid_dim)\n",
    "        # (batch_size, 1, src_len)\n",
    "        attention = torch.matmul(hidden , encoder_outputs.permute(1, 2, 0))\n",
    "        attention = attention.squeeze(1)  # (batch_size, src_len)\n",
    "\n",
    "        attention = attention.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Svj0D91vrq"
   },
   "source": [
    "## Bi-directional Encoder(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 45561,
     "status": "ok",
     "timestamp": 1612249401595,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "L8AVtNly1opU"
   },
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\r\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout_rate):\r\n",
    "        super(RNNEncoder, self).__init__()\r\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n",
    "\r\n",
    "        # bi-directional GRU encoder\r\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\r\n",
    "        \r\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n",
    "        self.dropout = nn.Dropout(dropout_rate)\r\n",
    "        \r\n",
    "    def forward(self, src, src_len):\r\n",
    "        '''\r\n",
    "        Args:\r\n",
    "            src: [src_len, batch_size]\r\n",
    "            src_len: [batch_size]\r\n",
    "        Returns:\r\n",
    "            outputs: [src_len, batch_size, enc_hid_dim * 2]\r\n",
    "            hidden: [batch_size, dec_hid_dim]\r\n",
    "        '''\r\n",
    "        # (src_len, batch_size, emb_dim)\r\n",
    "        embedded = self.dropout(self.embedding(src))\r\n",
    "                \r\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_len)\r\n",
    "        # hidden: [n_layers * num_directions, batch_size, hid_dim]\r\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\r\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs) \r\n",
    "        # (src_len, batch_size, hid_dim * num_directions)\r\n",
    "        \r\n",
    "        # hidden[-2, :, : ] 最後一層 forwards RNN\r\n",
    "        # hidden[-1, :, : ] 最後一層 backwards RNN\r\n",
    "        hidden = torch.tanh(\r\n",
    "            self.fc(self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\r\n",
    "        )\r\n",
    "\r\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVVFtNZv1-zA"
   },
   "source": [
    "## RNN Decoder(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 45555,
     "status": "ok",
     "timestamp": 1612249401595,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "P-W2XwE51_IF"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\r\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout_rate, attention):\r\n",
    "        super(RNNDecoder, self).__init__()\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n",
    "        self.attention = attention\r\n",
    "        \r\n",
    "        # GRU decoder\r\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n",
    "        \r\n",
    "        self.fc_out = nn.Linear(\r\n",
    "            (enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim\r\n",
    "        )\r\n",
    "        self.dropout = nn.Dropout(dropout_rate)\r\n",
    "        \r\n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\r\n",
    "        '''\r\n",
    "            input: [batch_size]\r\n",
    "            hidden: [batch_size, dec_hid_dim]\r\n",
    "            encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\r\n",
    "            mask: [batch_size, src_len]\r\n",
    "        '''\r\n",
    "        input = input.unsqueeze(0)  # (1, batch_size)\r\n",
    "        # (1, batch_size, emb_dim)\r\n",
    "        embedded = self.dropout(self.embedding(input))\r\n",
    "        \r\n",
    "        # (batch_size, src_len)\r\n",
    "        att = self.attention(hidden, encoder_outputs, mask)\r\n",
    "        att = att.unsqueeze(1)  # (batch_size, 1, src_len)\r\n",
    "\r\n",
    "        # (batch_size, src_len, enc_hid_dim * 2)\r\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
    "        # (batch_size, 1, enc_hid_dim * 2)\r\n",
    "        weighted = torch.bmm(att, encoder_outputs)\r\n",
    "        weighted = weighted.permute(1, 0, 2)  # (1, batch_size, enc_hid_dim * 2)\r\n",
    "        \r\n",
    "        # (1, batch_size, (enc_hid_dim * 2) + emb_dim)\r\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\r\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n",
    "        # output: [seq_len, batch_size, dec_hid_dim * n_directions]\r\n",
    "        # hidden: [n_layers * n_directions, batch_size, dec_hid_dim]\r\n",
    "        \r\n",
    "        # seq_len, n_layers and n_directions will always be 1 in this decoder, therefore:\r\n",
    "        # output: [1, batch_size, dec_hid_dim]\r\n",
    "        # hidden: [1, batch_size, dec_hid_dim]\r\n",
    "        # this also means that output == hidden\r\n",
    "        assert (output == hidden).all()\r\n",
    "        \r\n",
    "        embedded = embedded.squeeze(0)\r\n",
    "        output = output.squeeze(0)\r\n",
    "        weighted = weighted.squeeze(0)\r\n",
    "        \r\n",
    "        # (batch_size, output_dim)\r\n",
    "        prediction = self.fc_out(self.dropout(torch.cat((output, weighted, embedded), dim=1)))\r\n",
    "                \r\n",
    "        return prediction, hidden.squeeze(0), att.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiytVpfk4isG"
   },
   "source": [
    "## Seq2Seq 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 45547,
     "status": "ok",
     "timestamp": 1612249401596,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "TNzgZqHcS2CX"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqATTN(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super(Seq2SeqATTN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
    "        '''\n",
    "            src: [src_len, batch_size]\n",
    "            src_len: [batch_size]\n",
    "            trg: [trg_len, batch_size]\n",
    "            teacher_forcing_ratio is probability to use teacher forcing\n",
    "            e.g. if teacher_forcing_ratio is 0.75, we use teacher forcing 75% of the time\n",
    "        '''\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        mask = self.create_mask(src)  # (batch_size, src_len)\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state, all encoder hidden states and mask\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW2KIxhxrMGf"
   },
   "source": [
    "## 建立模型和重要參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55992,
     "status": "ok",
     "timestamp": 1612249412047,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "ybIY0kKGS_gI",
    "outputId": "feff84ad-f372-40ee-ab80-0c23ca13500b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型全部參數量: 15,511,486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqATTN(\n",
       "  (encoder): RNNEncoder(\n",
       "    (embedding): Embedding(12298, 256)\n",
       "    (rnn): GRU(256, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): RNNDecoder(\n",
       "    (embedding): Embedding(6078, 256)\n",
       "    (attention): Attention()\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=6078, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 256  # 注意 encoder hidden dim 設定必須為 decoder 的一半 \n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "def initial_mdl_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(initial_mdl_weights)\n",
    "print(f\"模型全部參數量: {sum(p.numel() for p in model.parameters()):10,d}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrjwJTlB70h7"
   },
   "source": [
    "## train and evalutate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 55987,
     "status": "ok",
     "timestamp": 1612249412048,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "KOpjxQJmTDYU"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg  # (trg_len, batch_size)\n",
    "        \n",
    "        # (trg_len, batch_size, output_dim)\n",
    "        output = model(src, src_len.cpu() , trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # ((trg_len - 1) * batch_size, output_dim)\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)  # ((trg_len - 1) * batch_size)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg  # (trg_len, batch_size)\n",
    "\n",
    "            # (trg_len, batch_size, output_dim)\n",
    "            output = model(src, src_len.cpu(), trg, 0)  # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # ((trg len - 1) * batch_size, output_dim)\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)  # ((trg len - 1) * batch_size)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFZUwxJA8CF6"
   },
   "source": [
    "## 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338483,
     "status": "ok",
     "timestamp": 1612249694549,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "bcmz65KO8FF1",
    "outputId": "179a5c0d-ec0a-4a4e-d872-5b1c7c5cf5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 training time: 13.68 sec, Training Loss: 5.440, Valiation Loss: 4.997\n",
      "Epoch  1 training time: 13.52 sec, Training Loss: 4.614, Valiation Loss: 4.755\n",
      "Epoch  2 training time: 13.63 sec, Training Loss: 4.232, Valiation Loss: 4.542\n",
      "Epoch  3 training time: 13.81 sec, Training Loss: 3.931, Valiation Loss: 4.372\n",
      "Epoch  4 training time: 13.88 sec, Training Loss: 3.650, Valiation Loss: 4.249\n",
      "Epoch  5 training time: 14.03 sec, Training Loss: 3.412, Valiation Loss: 4.050\n",
      "Epoch  6 training time: 13.96 sec, Training Loss: 3.159, Valiation Loss: 3.935\n",
      "Epoch  7 training time: 13.96 sec, Training Loss: 2.955, Valiation Loss: 3.879\n",
      "Epoch  8 training time: 13.88 sec, Training Loss: 2.776, Valiation Loss: 3.815\n",
      "Epoch  9 training time: 13.91 sec, Training Loss: 2.603, Valiation Loss: 3.833\n",
      "Epoch 10 training time: 13.73 sec, Training Loss: 2.453, Valiation Loss: 3.774\n",
      "Epoch 11 training time: 13.91 sec, Training Loss: 2.280, Valiation Loss: 3.750\n",
      "Epoch 12 training time: 13.89 sec, Training Loss: 2.153, Valiation Loss: 3.749\n",
      "Epoch 13 training time: 13.94 sec, Training Loss: 2.066, Valiation Loss: 3.728\n",
      "Epoch 14 training time: 13.82 sec, Training Loss: 1.983, Valiation Loss: 3.715\n",
      "Epoch 15 training time: 13.90 sec, Training Loss: 1.868, Valiation Loss: 3.712\n",
      "Epoch 16 training time: 13.85 sec, Training Loss: 1.791, Valiation Loss: 3.758\n",
      "Epoch 17 training time: 13.81 sec, Training Loss: 1.721, Valiation Loss: 3.821\n",
      "Epoch 18 training time: 13.88 sec, Training Loss: 1.646, Valiation Loss: 3.717\n",
      "Epoch 19 training time: 14.00 sec, Training Loss: 1.588, Valiation Loss: 3.772\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 20\r\n",
    "CLIP = 5\r\n",
    "model_dir = 'model'\r\n",
    "os.makedirs(model_dir, exist_ok=True)\r\n",
    "\r\n",
    "best_valid_loss = 9999999\r\n",
    "for epoch in range(MAX_EPOCHS):\r\n",
    "    start_time = time.time()\r\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
    "    end_time = time.time()\r\n",
    "    \r\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, f\"model-{epoch}.pt\"))\r\n",
    "    if valid_loss < best_valid_loss:\r\n",
    "        best_valid_loss = valid_loss\r\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, 'best-model.pt'))\r\n",
    "   \r\n",
    "    print(f\"Epoch {epoch:2d} training time: {end_time - start_time:.2f} sec, \"\r\n",
    "          f\"Training Loss: {train_loss:.3f}, Valiation Loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9F-sWnV9f28"
   },
   "source": [
    "## Evaluate testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338742,
     "status": "ok",
     "timestamp": 1612249694816,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "Ukg9t_iOTHlG",
    "outputId": "bcc096f0-219b-4ef6-98c9-c36e2198fbf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.797 | Test PPL:  44.564 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'best-model.pt')))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 338737,
     "status": "ok",
     "timestamp": 1612249694817,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "Q-caE1Y1TL5p"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval()\n",
    "        \n",
    "    tokens = [token.lower() for token in sentence]\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
    "\n",
    "    mask = model.create_mask(src_tensor)\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "        attentions[i] = attention\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JxlvTVD-kvT"
   },
   "source": [
    "## 選擇一句來看翻譯結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339041,
     "status": "ok",
     "timestamp": 1612249695125,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "b22YJ_44-l_O",
    "outputId": "d772934a-d747-494a-90e3-8188e452c41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['白天', '越', '來', '越', '長', '了']\n",
      "trg = ['days', 'are', 'getting', 'longer', '.']\n",
      "predicted trg = ['the', 'days', 'are', 'getting', 'longer', 'longer', 'longer', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 123\r\n",
    "\r\n",
    "src = vars(train_dataset.examples[example_idx])['src']\r\n",
    "print(f'src = {src}')\r\n",
    "trg = vars(train_dataset.examples[example_idx])['trg']\r\n",
    "print(f'trg = {trg}')\r\n",
    "\r\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWpOXdiL_c9A"
   },
   "source": [
    "## BLEU score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 339037,
     "status": "ok",
     "timestamp": 1612249695126,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "vgHrm83K_dNX"
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len=50):    \r\n",
    "    trgs, pred_trgs = [], []\r\n",
    "    for datum in data:\r\n",
    "        src = vars(datum)['src']\r\n",
    "        trg = vars(datum)['trg']\r\n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\r\n",
    "        \r\n",
    "        # cut off <eos> token\r\n",
    "        pred_trg = pred_trg[:-1]\r\n",
    "        \r\n",
    "        pred_trgs.append(pred_trg)\r\n",
    "        trgs.append([trg])\r\n",
    "        \r\n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360454,
     "status": "ok",
     "timestamp": 1612249716547,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "SUvCDups_mdB",
    "outputId": "b637805c-f4db-4052-8f8d-d8c803911c6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 17.86\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_dataset, SRC, TRG, model, device)\r\n",
    "print(f'BLEU score = {bleu_score * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 360449,
     "status": "ok",
     "timestamp": 1612249716548,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "9jhh_5_SYYLT"
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    src = ['<sos>'] + [t.lower() for t in sentence] + ['<eos>']\n",
    "    trg = translation\n",
    "    att = attention.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "    df = pd.DataFrame(data=att, index=trg, columns=src)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "    sns.heatmap(df, vmin=0, vmax=1.0, ax=ax, cmap=\"Blues\", cbar=False)\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.setp(label_y, rotation=360, ha='right', fontproperties=myfont)\n",
    "    label_x = ax.get_xticklabels()\n",
    "    plt.setp(label_x, rotation=90, ha='right', fontproperties=myfont)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4n7915Mrcs1"
   },
   "source": [
    "## 作業重點\n",
    "請選擇一個好的翻譯結果並將其 Attention 視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360445,
     "status": "ok",
     "timestamp": 1612249716548,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "pRYXjqgvYb-E",
    "outputId": "cf7c3a3f-0034-49e5-cee2-8231f6bd3a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['这是', '我', '读', '过', '的', '书', '里面', '写', '的', '最', '差', '的', '一', '本', '了']\n",
      "trg = ['this', 'is', 'the', 'worst', 'book', 'i', \"'ve\", 'ever', 'read', '.']\n",
      "predicted trg = ['this', 'is', 'the', 'book', 'book', 'i', \"'ve\", 'ever', 'read', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# 請在這自行調整 sample index 觀察不同句子的 Attention 結果\n",
    "example_idx = 456\n",
    "\n",
    "src = vars(train_dataset.examples[example_idx])['src']\n",
    "print(f'src = {src}')\n",
    "trg = vars(train_dataset.examples[example_idx])['trg']\n",
    "print(f'trg = {trg}')\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cdlptGJrsfv"
   },
   "source": [
    "# 觀察翻譯文和被翻譯文的語意對應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 360775,
     "status": "ok",
     "timestamp": 1612249716885,
     "user": {
      "displayName": "王俊煒",
      "photoUrl": "",
      "userId": "10865038187423164500"
     },
     "user_tz": -480
    },
    "id": "H8f8csPSYfkU",
    "outputId": "133bac91-d597-479e-967c-2a8999a22985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是我读过的书里面写的最差的一本了\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(''.join(src))\n",
    "display_attention(src, translation, attention)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
