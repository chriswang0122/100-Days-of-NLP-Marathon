{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的:了解決策樹的節點分支依據\n",
    "本次作業可參考簡報中的延伸閱讀[訊息增益](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "若你是決策樹，下列兩種分類狀況(a,b)，你會選擇哪種做分類？為什麼？\n",
    "\n",
    "<img src='hw_1.png' style='width:500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "- **Entropy**:\n",
    "$$\\begin{aligned}\n",
    "I_H\\left(D_p\\right) &= -\\left(\\frac{20}{40}\\log_2\\left(\\frac{20}{40}\\right) + \\frac{20}{40}\\log_2\\left(\\frac{20}{40}\\right)\\right) = 1 \\\\\n",
    "\\\\\n",
    "A: I_H(D_{\\text{left}}) &= -\\left(\\frac{15}{20}\\log_2\\left(\\frac{15}{20}\\right) + \\frac{5}{20}\\log_2\\left(\\frac{5}{20}\\right)\\right) = 0.8113 \\\\\n",
    "A: I_H(D_{\\text{right}}) &= -\\left(\\frac{5}{20}\\log_2\\left(\\frac{5}{20}\\right) + \\frac{15}{20}\\log_2\\left(\\frac{15}{20}\\right)\\right) = 0.8113 \\\\\n",
    "A: IG_H &= 1 - \\frac{20}{40}\\times 0.8113 -\\frac{20}{40}\\times 0.8113 = 0.1887 \\\\\n",
    "\\\\\n",
    "B: I_H(D_{\\text{left}}) &= -\\left(\\frac{15}{35}\\log_2\\left(\\frac{15}{35}\\right) + \\frac{20}{35}\\log_2\\left(\\frac{20}{35}\\right)\\right) = 0.9852 \\\\\n",
    "B: I_H(D_{\\text{right}}) &= -\\left(\\frac{5}{5}\\log_2\\left(\\frac{5}{5}\\right) + \\frac{0}{5}\\log_2\\left(\\frac{0}{5}\\right)\\right) = 0 \\\\\n",
    "B: IG_H &= 1 - \\frac{35}{40}\\times 0.9852 -\\frac{5}{40}\\times 0 = 0.1380\n",
    "\\end{aligned}$$\n",
    "\n",
    "- **Gini**:\n",
    "$$\\begin{aligned}\n",
    "I_G\\left(D_p\\right) &= 1 - \\left(\\left(\\frac{20}{40}\\right)^2 + \\left(\\frac{20}{40}\\right)^2\\right) = 0.5 \\\\\n",
    "\\\\\n",
    "A: I_G(D_{\\text{left}}) &= 1 -\\left(\\left(\\frac{15}{20}\\right)^2 + \\left(\\frac{5}{20}\\right)^2\\right) = 0.375 \\\\\n",
    "A: I_G(D_{\\text{right}}) &= 1 -\\left(\\left(\\frac{5}{20}\\right)^2 + \\left(\\frac{15}{20}\\right)^2\\right) = 0.375 \\\\\n",
    "A: IG_G &= 0.5 - \\frac{20}{40}\\times 0.375 -\\frac{20}{40}\\times 0.375 = 0.125 \\\\\n",
    "\\\\\n",
    "B: I_G(D_{\\text{left}}) &= 1 -\\left(\\left(\\frac{15}{35}\\right)^2 + \\left(\\frac{20}{35}\\right)^2\\right) = 0.4898 \\\\\n",
    "B: I_G(D_{\\text{right}}) &= 1 -\\left(\\left(\\frac{5}{5}\\right)^2 + \\left(\\frac{0}{5}\\right)^2\\right) = 0 \\\\\n",
    "B: IG_G &= 0.5 - \\frac{35}{40}\\times 0.4898 -\\frac{5}{40}\\times 0 = 0.0714\n",
    "\\end{aligned}$$\n",
    "\n",
    "不管是 Entropy 或 Gini，(a)分類狀況都偏較好<br/>\n",
    "∵ $A:IG_H > B:IG_H$ or $A:IG_G > B:IG_G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閱讀作業\n",
    "\n",
    "決策樹根據計算分割準則的不同(ex: Entropy, Gini, Gain ratio)，可分為ID3, C4.5, CART樹的算法，請同學閱讀下列文章，來更加了解決策樹的算法。\n",
    "\n",
    "[決策樹(ID3, C4.5, CART)](https://blog.csdn.net/u010089444/article/details/53241218)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
